quiero implementar esta api a mi proyecto en next.js, la idea es que no solo me mostrar en texto la respuesta de la api, si no que el text to speech me la lea, esta es la documentación: Convert text to speech (streaming)

If you prefer to stream the audio directly without saving it to a file, you can use our streaming feature.

import { ElevenLabsClient } from "elevenlabs";
import * as dotenv from "dotenv";

dotenv.config();

const ELEVENLABS_API_KEY = process.env.ELEVENLABS_API_KEY;

if (!ELEVENLABS_API_KEY) {
  throw new Error("Missing ELEVENLABS_API_KEY in environment variables");
}

const client = new ElevenLabsClient({
  apiKey: ELEVENLABS_API_KEY,
});

export const createAudioStreamFromText = async (
  text: string
): Promise<Buffer> => {
  const audioStream = await client.generate({
    voice: "Rachel",
    model_id: "eleven_turbo_v2_5",
    text,
  });

  const chunks: Buffer[] = [];
  for await (const chunk of audioStream) {
    chunks.push(chunk);
  }

  const content = Buffer.concat(chunks);
  return content;
};

You can then run this function with:


TypeScript

await createAudioStreamFromText("This is James");   y de mi proeycto nextjs ese es el usePlayer.ts: import { useRef, useState } from "react";

export function usePlayer() {
	const [isPlaying, setIsPlaying] = useState(false);
	const audioContext = useRef<AudioContext | null>(null);
	const source = useRef<AudioBufferSourceNode | null>(null);

	async function play(stream: ReadableStream, callback: () => void) {
		stop();
		audioContext.current = new AudioContext({ sampleRate: 24000 });

		let nextStartTime = audioContext.current.currentTime;
		const reader = stream.getReader();
		let leftover = new Uint8Array();
		let result = await reader.read();
		setIsPlaying(true);

		while (!result.done && audioContext.current) {
			const data = new Uint8Array(leftover.length + result.value.length);
			data.set(leftover);
			data.set(result.value, leftover.length);

			const length = Math.floor(data.length / 4) * 4;
			const remainder = data.length % 4;
			const buffer = new Float32Array(data.buffer, 0, length / 4);

			leftover = new Uint8Array(data.buffer, length, remainder);

			const audioBuffer = audioContext.current.createBuffer(
				1,
				buffer.length,
				audioContext.current.sampleRate
			);
			audioBuffer.copyToChannel(buffer, 0);

			source.current = audioContext.current.createBufferSource();
			source.current.buffer = audioBuffer;
			source.current.connect(audioContext.current.destination);
			source.current.start(nextStartTime);

			nextStartTime += audioBuffer.duration;

			result = await reader.read();
			if (result.done) {
				source.current.onended = () => {
					stop();
					callback();
				};
			}
		}
	}

	function stop() {
		audioContext.current?.close();
		audioContext.current = null;
		setIsPlaying(false);
	}

	return {
		isPlaying,
		play,
		stop,
	};
} route.ts: import Groq from "groq-sdk";
import { headers } from "next/headers";
import { z } from "zod";
import { zfd } from "zod-form-data";

const groq = new Groq();

const schema = zfd.formData({
  // input: z.union([zfd.text(), zfd.file()]),
  input: z.union([zfd.text(), z.any()]),
  message: zfd.repeatableOfType(
    zfd.json(
      z.object({
        role: z.enum(["user", "assistant"]),
        content: z.string(),
      })
    )
  ),
});

export async function POST(request: Request) {
  console.time("transcribe " + request.headers.get("x-vercel-id") || "local");

  const { data, success } = schema.safeParse(await request.formData());
  if (!success) return new Response("Invalid request", { status: 400 });
  
  let transcript: string;
  if (data.input instanceof File) {
    const result = await getTranscript(data.input);
    if (!result) return new Response("Invalid audio", { status: 400 });
    transcript = result;
  } else {
    transcript = data.input;
  }


  console.timeEnd(
    "transcribe " + request.headers.get("x-vercel-id") || "local"
  );
  console.time(
    "text completion " + request.headers.get("x-vercel-id") || "local"
  );

  const completion = await groq.chat.completions.create({
    model: "llama3-8b-8192",
    messages: [
      {
        role: "system",
        content: - You are Echo, a friendly and helpful voice assistant.
        - Respond briefly to the user's request, and do not provide unnecessary information.
        - If you don't understand the user's request, ask for clarification.
        - You do not have access to up-to-date information, so you should not provide real-time data.
        - You are not capable of performing actions other than responding to the user.
        - Do not use markdown, emojis, or other formatting in your responses. Respond in a way easily spoken by text-to-speech software.
        - User location is ${location()}.
        - The current time is ${time()}.
        - Your large language model is Llama 3, created by Meta, the 8 billion parameter version. It is hosted on Groq, an AI infrastructure company that builds fast inference technology.
        - Your text-to-speech model is Sonic, created and hosted by Cartesia, a company that builds fast and realistic speech synthesis technology.
        - You are built with Next.js and hosted on Vercel.,
      },
      ...data.message,
      {
        role: "user",
        content: transcript,
      },
    ],
  });

  const response = completion.choices[0].message.content;
  console.timeEnd(
    "text completion " + request.headers.get("x-vercel-id") || "local"
  );

  return new Response(JSON.stringify({ text: response }), {
    headers: {
      "Content-Type": "application/json",
    },
  });
}

function location() {
  const headersList = headers();

  const country = headersList.get("x-vercel-ip-country");
  const region = headersList.get("x-vercel-ip-country-region");
  const city = headersList.get("x-vercel-ip-city");

  if (!country || !region || !city) return "unknown";

  return ${city}, ${region}, ${country};
}

function time() {
  return new Date().toLocaleString("en-US", {
    timeZone: headers().get("x-vercel-ip-timezone") || undefined,
  });
}

async function getTranscript(input: File) {
  try {
    const { text } = await groq.audio.transcriptions.create({
      file: input,
      model: "whisper-large-v3",
    });

    return text.trim() || null;
  } catch {
    return null;
  }
} y page.tsx: "use client";

import { useState, useRef, useEffect } from "react";
import { EnterIcon, LoadingIcon } from "@/app/lib/icons";
import { toast } from "sonner";
import { useMicVAD, utils } from "@ricky0123/vad-react";

type Message = {
  role: "user" | "assistant";
  content: string;
  latency?: number;
};

export default function Home() {
  const [input, setInput] = useState("");
  const inputRef = useRef<HTMLInputElement>(null);
  const [messages, setMessages] = useState<Message[]>([]);
  const [isPending, setIsPending] = useState(false);

  const vad = useMicVAD({
    startOnLoad: true,
    workletURL: "/vad.worklet.bundle.min.js",
    modelURL: "/silero_vad.onnx",
    positiveSpeechThreshold: 0.6,
    minSpeechFrames: 4,
    ortConfig: (ort) => {
      const isSafari = /^((?!chrome|android).)*safari/i.test(navigator.userAgent);
      ort.env.wasm = {
        wasmPaths: {
          "ort-wasm-simd-threaded.wasm": "/ort-wasm-simd-threaded.wasm",
          "ort-wasm-simd.wasm": "/ort-wasm-simd.wasm",
          "ort-wasm.wasm": "/ort-wasm.wasm",
          "ort-wasm-threaded.wasm": "/ort-wasm-threaded.wasm",
        },
        numThreads: isSafari ? 1 : 4,
      };
    },
    onSpeechEnd: (audio) => {
      const wav = utils.encodeWAV(audio);
      const blob = new Blob([wav], { type: "audio/wav" });
      submit(blob);
      const isFirefox = navigator.userAgent.includes("Firefox");
      if (isFirefox) vad.pause();
    },
  });

  useEffect(() => {
    function keyDown(e: KeyboardEvent) {
      if (e.key === "Enter") return inputRef.current?.focus();
      if (e.key === "Escape") return setInput("");
    }

    window.addEventListener("keydown", keyDown);
    return () => window.removeEventListener("keydown", keyDown);
  }, []);

  const submit = async (data: string | Blob) => {
    const formData = new FormData();
    if (typeof data === "string") {
      formData.append("input", data);
    } else {
      formData.append("input", data, "audio.wav");
    }

    for (const message of messages) {
      formData.append("message", JSON.stringify(message));
    }

    setIsPending(true);
    const submittedAt = Date.now();

    try {
      const response = await fetch("/api", {
        method: "POST",
        body: formData,
      });

      const responseData = await response.json();

      if (!response.ok || !responseData.text) {
        if (response.status === 429) {
          toast.error("Too many requests. Please try again later.");
        } else {
          toast.error(responseData.error || "An error occurred.");
        }

        setIsPending(false);
        return;
      }

      const latency = Date.now() - submittedAt;

      setInput("");

      setMessages((prevMessages) => [
        ...prevMessages,
        {
          role: "user",
          content: data instanceof Blob ? "Audio input received" : data,
        },
        {
          role: "assistant",
          content: responseData.text,
          latency,
        },
      ]);

    } catch (error) {
      console.error("Error submitting data:", error);
      toast.error("An error occurred while submitting your request.");
    } finally {
      setIsPending(false);
    }
  };

  function handleFormSubmit(e: React.FormEvent) {
    e.preventDefault();
    submit(input);
  }

  return (
    <main className="flex min-h-screen flex-col items-center justify-center gap-2 grow">
      <h1 className="text-5xl text-gray-400">ECHO</h1>

      <form
        className="rounded-full bg-neutral-200/80 dark:bg-neutral-800/80 flex items-center w-full max-w-3xl border border-transparent hover:border-neutral-300 focus-within:border-neutral-400 hover:focus-within:border-neutral-400 dark:hover:border-neutral-700 dark:focus-within:border-neutral-600 dark:hover:focus-within:border-neutral-600"
        onSubmit={handleFormSubmit}
      >
        <input
          type="text"
          className="bg-transparent focus:outline-none p-4 w-full placeholder:text-neutral-600 dark:placeholder:text-neutral-400 text-white"
          required
          placeholder="Ask me anything"
          value={input}
          onChange={(e) => setInput(e.target.value)}
          ref={inputRef}
        />

        <button
          type="submit"
          className="p-4 text-neutral-700 hover:text-black dark:text-neutral-300 dark:hover:text-white"
          disabled={isPending}
          aria-label="Submit"
        >
          {isPending ? <LoadingIcon /> : <EnterIcon />}
        </button>
      </form>

      <div className="text-neutral-300 dark:text-neutral-400 pt-4 text-center max-w-xl text-balance min-h-28 space-y-4">
        {messages.length > 0 ? (
          <p>
            {messages.at(-1)?.content}
            <span className="text-xs font-mono text-neutral-300 dark:text-neutral-400">
              {" "}
              ({messages.at(-1)?.latency}ms)
            </span>
          </p>
        ) : (
          <p>
            A fast AI assistant with voice recognition and text input
          </p>
        )}
        {vad.loading ? (
          <p>Loading speech detection...</p>
        ) : vad.errored ? (
          <p>Failed to load speech detection.</p>
        ) : (
          <p>{vad.userSpeaking ? "User is speaking" : "Start talking to chat"}</p>
        )}
      </div>
    </main>
  );
}   por favor DAME LOS CÓDIGOS COMPLETOS, DE MIS ARCHIVOS CON LAS MODIFICACIONES, QUE NO TE FALTE NADA, Y ARCHIVOS QUE SEAN NECESARIOS CREAR